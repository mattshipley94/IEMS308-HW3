{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import csv\n",
    "import plotly as py\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pandas import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# texts 2013 holds all 365 days worth of news articles from the year 2013\n",
    "texts2013 = []\n",
    "location = 'D:\\\\Homework3\\\\2013'\n",
    "for filename in os.listdir(location):\n",
    "    fullfilename = location + '\\\\' + filename\n",
    "    with open(fullfilename, 'r', encoding='latin-1') as myfile:\n",
    "        text = myfile.read()\n",
    "        texts2013.append(text)\n",
    "        myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# texts 2014 holds all 365 days worth of news articles from the year 2014\n",
    "texts2014 = []\n",
    "location = 'D:\\\\Homework3\\\\2014'\n",
    "for filename in os.listdir(location):\n",
    "    fullfilename = location + '\\\\' + filename\n",
    "    with open(fullfilename, 'r', encoding='latin-1') as myfile:\n",
    "        text = myfile.read()\n",
    "        texts2014.append(text)\n",
    "        myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reads in training datasets\n",
    "ceosTraining = pd.read_csv('D:\\\\Homework3\\\\all\\ceoEdited.csv', delimiter=',', encoding='latin-1', header=None,names=[\"CEO Training\"])\n",
    "companiesTraining = pd.read_csv('D:\\\\Homework3\\\\all\\companiesEdited.csv', delimiter=',', encoding='latin-1', header=None,names=[\"Companies Training\"])\n",
    "percentTraining = pd.read_csv('D:\\\\Homework3\\\\all\\percentage.csv', delimiter=',', encoding='latin-1', header=None,names=[\"Percentage Training\"])\n",
    "companySuffixes = pd.read_csv('D:\\\\Homework3\\\\all\\companySuffixes.csv', delimiter=',', header=None,names=[\"Suffixes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we only want sentences where RegEx finds \"Firstname Lastname\", so we can isolate the sample\n",
    "# firstLastSent is a collection of sentences with Firstname Lastname pattern, stop words removed\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "firstLastSent = []\n",
    "\n",
    "for newsDate in np.arange(len(texts2013)):\n",
    "    currentText = texts2013[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    for j in np.arange(len(currentSentences)):\n",
    "        thisSentence = currentSentences[j]\n",
    "        m = re.search(rC,thisSentence)\n",
    "        if (m != None):\n",
    "            thisSentence = nltk.word_tokenize(thisSentence)\n",
    "            thisSentence = [word for word in thisSentence if not word in stop]\n",
    "            firstLastSent.append(thisSentence)\n",
    "            \n",
    "for newsDate in np.arange(len(texts2014)):\n",
    "    currentText = texts2014[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    for j in np.arange(len(currentSentences)):\n",
    "        thisSentence = currentSentences[j]\n",
    "        m = re.search(r\"[A-Z][a-z]* [A-Z][a-z]*\",thisSentence)\n",
    "        if (m != None):\n",
    "            thisSentence = nltk.word_tokenize(thisSentence)\n",
    "            thisSentence = [word for word in thisSentence if not word in stop]\n",
    "            firstLastSent.append(thisSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates trainingSample, used to train the classification model for CEOs\n",
    "# we know approximately 18,000 sentences have CEO names, 230,000 have FirstName LastName\n",
    "# select all sentences with CEO names, and 10% of sentences without\n",
    "trainingSample = []\n",
    "\n",
    "for i in np.arange(len(firstLastSent)):\n",
    "    thisSentence = firstLastSent[i]\n",
    "    selectSentence = 0\n",
    "    for j in np.arange(len(thisSentence)-1):\n",
    "        coupling = thisSentence[j] + \" \" + thisSentence[j+1]\n",
    "        if (coupling in ceosTraining['CEO Training'].values):\n",
    "            selectSentence = 1\n",
    "    if ((selectSentence == 1) or (np.random.uniform() < .1)):\n",
    "        trainingSample.append(thisSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code finds how many sentences are in corpus\n",
    "sentencesIn2013 = 0\n",
    "for newsDate in np.arange(len(texts2013)):\n",
    "    currentText = texts2013[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    currentNumber = len(currentSentences)\n",
    "    sentencesIn2013 = sentencesIn2013 + currentNumber\n",
    "\n",
    "sentencesIn2014 = 0\n",
    "for newsDate in np.arange(len(texts2014)):\n",
    "    currentText = texts2014[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    currentNumber = len(currentSentences)\n",
    "    sentencesIn2014 = sentencesIn2014 + currentNumber\n",
    "    \n",
    "corpusSentences = sentencesIn2013 + sentencesIn2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Token = []\n",
    "Coupling = []\n",
    "AlphaNum = []\n",
    "PrevCap = []\n",
    "Title = []\n",
    "NextCap = []\n",
    "Position = []\n",
    "FirstLength = []\n",
    "SecondLength = []\n",
    "FirstNoun = []\n",
    "SecondNoun = []\n",
    "CEOinSent = []\n",
    "IsCEO = []\n",
    "\n",
    "for sent in np.arange(len(trainingSample)):\n",
    "    sentence = trainingSample[sent]\n",
    "    sentencePOS = nltk.pos_tag(sentence)\n",
    "    sentLength = len(sentence)\n",
    "    \n",
    "    for i in np.arange(sentLength):\n",
    "        if (sentence[i].lower() == \"ceo\"):\n",
    "            thisCEOinSent = True\n",
    "        elif (i < sentLength - 2) and (sentence[i].lower() == \"chief\") and (sentence[i+1].lower() == \"executive\") and (sentence[i+2].lower() == \"officer\"):\n",
    "            thisCEOinSent = True\n",
    "        else:\n",
    "            thisCEOinSent = False\n",
    "            \n",
    "    for word in np.arange(sentLength):\n",
    "        if sentence[word].istitle():\n",
    "            thisToken = sentence[word]\n",
    "            Token.append(thisToken)\n",
    "\n",
    "            if (word == sentLength - 1):\n",
    "                thisCoupling = thisToken\n",
    "                Coupling.append(thisCoupling)\n",
    "                AlphaNum.append(thisToken.isalpha())\n",
    "                SecondLength.append(False)\n",
    "                SecondPOS = False\n",
    "            else:\n",
    "                thisCoupling = thisToken + \" \" + sentence[word+1]\n",
    "                Coupling.append(thisCoupling)\n",
    "                AlphaNum.append(thisToken.isalpha() and sentence[word+1].isalpha())\n",
    "                SecondLength.append(len(sentence[word+1]))\n",
    "                SecondPOS = sentencePOS[word+1][1]\n",
    "\n",
    "            if (word==0):\n",
    "                PrevCap.append(False)\n",
    "                #PrevPOS = False\n",
    "            else:\n",
    "                PrevCap.append(sentence[word-1].istitle())\n",
    "                #PrevPOS = sentencePOS[word-1][1]\n",
    "\n",
    "            if thisCoupling.istitle():\n",
    "                Title.append(True)\n",
    "            else:\n",
    "                Title.append(False)\n",
    "\n",
    "            if (word >= (sentLength-2) or (not sentence[word+2].istitle())):\n",
    "                NextCap.append(False)\n",
    "            else:\n",
    "                NextCap.append(True)\n",
    "\n",
    "            #if (word >= sentLength - 2):\n",
    "            #    NextPOS = False\n",
    "            #else:\n",
    "            #    NextPOS = sentencePOS[word+2][1]\n",
    "\n",
    "            Position.append(word)\n",
    "            FirstLength.append(len(thisToken))\n",
    "            FirstPOS = sentencePOS[word][1]\n",
    "\n",
    "            if (FirstPOS == \"NNP\") or (FirstPOS == \"NN\"):\n",
    "                FirstNoun.append(True)\n",
    "            else:\n",
    "                FirstNoun.append(False)\n",
    "\n",
    "            if (SecondPOS == \"NNP\") or (SecondPOS == \"NN\"):\n",
    "                SecondNoun.append(True)\n",
    "            else:\n",
    "                SecondNoun.append(False)\n",
    "                \n",
    "            CEOinSent.append(thisCEOinSent)\n",
    "\n",
    "            if (thisCoupling in ceosTraining['CEO Training'].values):\n",
    "                IsCEO.append(True)\n",
    "            else:\n",
    "                IsCEO.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = np.asarray([PrevCap, Title, Position, FirstLength, SecondLength, NextCap, FirstNoun, SecondNoun, CEOinSent])\n",
    "trainLabels = IsCEO\n",
    "train = train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "ynb = clf.fit(train, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = ynb.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum((predicted == trainLabels))\n",
    "np.sum(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#index = [i for i, x in enumerate(trainLabels) if x == True]\n",
    "#CEOdata['FirstNoun'].values[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(firstLastSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect data on entire firstLastSent sample\n",
    "\n",
    "Token = []\n",
    "Coupling = []\n",
    "AlphaNum = []\n",
    "PrevCap = []\n",
    "Title = []\n",
    "NextCap = []\n",
    "Position = []\n",
    "FirstLength = []\n",
    "SecondLength = []\n",
    "FirstNoun = []\n",
    "SecondNoun = []\n",
    "CEOinSent = []\n",
    "IsCEO = []\n",
    "\n",
    "for sent in np.arange(len(firstLastSent)):\n",
    "    sentence = firstLastSent[sent]\n",
    "    sentencePOS = nltk.pos_tag(sentence)\n",
    "    sentLength = len(sentence)\n",
    "    \n",
    "    for i in np.arange(sentLength):\n",
    "        if (sentence[i].lower() == \"ceo\"):\n",
    "            thisCEOinSent = True\n",
    "        elif (i < sentLength - 2) and (sentence[i].lower() == \"chief\") and (sentence[i+1].lower() == \"executive\") and (sentence[i+2].lower() == \"officer\"):\n",
    "            thisCEOinSent = True\n",
    "        else:\n",
    "            thisCEOinSent = False\n",
    "            \n",
    "    for word in np.arange(sentLength):\n",
    "        if sentence[word].istitle():\n",
    "            thisToken = sentence[word]\n",
    "            Token.append(thisToken)\n",
    "\n",
    "            if (word == sentLength - 1):\n",
    "                thisCoupling = thisToken\n",
    "                Coupling.append(thisCoupling)\n",
    "                AlphaNum.append(thisToken.isalpha())\n",
    "                SecondLength.append(False)\n",
    "                SecondPOS = False\n",
    "            else:\n",
    "                thisCoupling = thisToken + \" \" + sentence[word+1]\n",
    "                Coupling.append(thisCoupling)\n",
    "                AlphaNum.append(thisToken.isalpha() and sentence[word+1].isalpha())\n",
    "                SecondLength.append(len(sentence[word+1]))\n",
    "                SecondPOS = sentencePOS[word+1][1]\n",
    "\n",
    "            if (word==0):\n",
    "                PrevCap.append(False)\n",
    "                #PrevPOS = False\n",
    "            else:\n",
    "                PrevCap.append(sentence[word-1].istitle())\n",
    "                #PrevPOS = sentencePOS[word-1][1]\n",
    "\n",
    "            if thisCoupling.istitle():\n",
    "                Title.append(True)\n",
    "            else:\n",
    "                Title.append(False)\n",
    "\n",
    "            if (word >= (sentLength-2) or (not sentence[word+2].istitle())):\n",
    "                NextCap.append(False)\n",
    "            else:\n",
    "                NextCap.append(True)\n",
    "\n",
    "            #if (word >= sentLength - 2):\n",
    "            #    NextPOS = False\n",
    "            #else:\n",
    "            #    NextPOS = sentencePOS[word+2][1]\n",
    "\n",
    "            Position.append(word)\n",
    "            FirstLength.append(len(thisToken))\n",
    "            FirstPOS = sentencePOS[word][1]\n",
    "\n",
    "            if (FirstPOS == \"NNP\") or (FirstPOS == \"NN\"):\n",
    "                FirstNoun.append(True)\n",
    "            else:\n",
    "                FirstNoun.append(False)\n",
    "\n",
    "            if (SecondPOS == \"NNP\") or (SecondPOS == \"NN\"):\n",
    "                SecondNoun.append(True)\n",
    "            else:\n",
    "                SecondNoun.append(False)\n",
    "                \n",
    "            CEOinSent.append(thisCEOinSent)\n",
    "\n",
    "            if (thisCoupling in ceosTraining['CEO Training'].values):\n",
    "                IsCEO.append(True)\n",
    "            else:\n",
    "                IsCEO.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.asarray([PrevCap, Title, Position, FirstLength, SecondLength, NextCap, FirstNoun, SecondNoun, CEOinSent])\n",
    "test = test.transpose()\n",
    "predicted = ynb.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FP = 0\n",
    "FN = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "FNarray = np.zeros(len(IsCEO))\n",
    "\n",
    "for i in np.arange(len(predicted)):\n",
    "    thisPredict = predicted[i]\n",
    "    thisActual = IsCEO[i]\n",
    "    if thisPredict and thisActual:\n",
    "        TP = TP + 1\n",
    "    elif thisPredict and (thisActual == False):\n",
    "        FP = FP + 1\n",
    "    elif (thisPredict == False) and thisActual:\n",
    "        FN = FN + 1\n",
    "    elif (thisPredict == False) and (thisActual == False):\n",
    "        TN = TN + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(FP) + \" false positives\\n\" + str(FN) + \" false negatives\\n\" + str(TP) + \" true positives\\n\" + str(TN) + \" true negatives\")\n",
    "print(\"\\naccuracy is \" + str((TP + TN)/(TP + TN + FP + FN)))\n",
    "print(\"precision is \" + str(TP/(TP + FP)))\n",
    "print(\"recall is \" + str(TP/(TP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedCEOs = []\n",
    "\n",
    "for i in np.arange(len(predicted)):\n",
    "    if (predicted[i] == True):\n",
    "        predictedCEOs.append(Coupling[i])\n",
    "\n",
    "# following function used from Markus Jarderot on StackOverflow\n",
    "# http://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-in-whilst-preserving-order\n",
    "def f7(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "CEOsList = f7(predictedCEOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputFile = open(\"./ShipleyPredictedCEOs.txt\", \"w\", encoding = 'latin-1')\n",
    "for item in CEOsList:\n",
    "    outputFile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXTRACT PERCENTAGES\n",
    "# We know that a percentage will have either the percent sign (%) or the word percent, extract these sentences\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "percentSent = []\n",
    "\n",
    "for newsDate in np.arange(len(texts2013)):\n",
    "    currentText = texts2013[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    for j in np.arange(len(currentSentences)):\n",
    "        thisSentence = currentSentences[j]\n",
    "        m = re.search(r\"\\%\",thisSentence)\n",
    "        n = re.search(r\"percent\",thisSentence)\n",
    "        if (m != None) or (n != None):\n",
    "            thisSentence = nltk.word_tokenize(thisSentence)\n",
    "            thisSentence = [word for word in thisSentence if not word in stop]\n",
    "            percentSent.append(thisSentence)\n",
    "            \n",
    "for newsDate in np.arange(len(texts2014)):\n",
    "    currentText = texts2014[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    for j in np.arange(len(currentSentences)):\n",
    "        thisSentence = currentSentences[j]\n",
    "        m = re.search(r\"\\%\",thisSentence)\n",
    "        n = re.search(r\"percent\",thisSentence)\n",
    "        if (m != None) or (n != None):\n",
    "            thisSentence = nltk.word_tokenize(thisSentence)\n",
    "            thisSentence = [word for word in thisSentence if not word in stop]\n",
    "            percentSent.append(thisSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates trainingSample, used to train the classification model for percentages\n",
    "trainingSample = []\n",
    "\n",
    "for i in np.arange(len(percentSent)):\n",
    "    if (np.random.uniform() < .2):\n",
    "        trainingSample.append(percentSent[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pToken = []\n",
    "pCoupling = []\n",
    "pFirstNoun = []\n",
    "pFirstAdj = []\n",
    "pFirstCD = []\n",
    "pSecondNoun = []\n",
    "pSecondPerc = []\n",
    "pSecondPercSymb = []\n",
    "pIsPercent = []\n",
    "\n",
    "for sent in np.arange(len(trainingSample)):\n",
    "    sentence = trainingSample[sent]\n",
    "    sentencePOS = nltk.pos_tag(sentence)\n",
    "    sentLength = len(sentence)\n",
    "            \n",
    "    for word in np.arange(sentLength-1):\n",
    "        thisToken = sentence[word]\n",
    "        nextToken = sentence[word+1]\n",
    "        pToken.append(thisToken)\n",
    "        \n",
    "        if (nextToken == \"%\"):\n",
    "            thisCoupling = thisToken + nextToken\n",
    "        else:\n",
    "            thisCoupling = thisToken + \" \" + nextToken\n",
    "        pCoupling.append(thisCoupling)\n",
    "        \n",
    "        FirstPOS = sentencePOS[word][1]\n",
    "        SecondPOS = sentencePOS[word+1][1]\n",
    "\n",
    "        pFirstNoun.append(FirstPOS == \"NN\")\n",
    "        pFirstAdj.append(FirstPOS == \"JJ\")\n",
    "        pFirstCD.append(FirstPOS == \"CD\")\n",
    "        pSecondNoun.append(SecondPOS == \"NN\")\n",
    "        pSecondPerc.append(nextToken.lower() == \"percent\")\n",
    "        pSecondPercSymb.append(nextToken == \"%\")\n",
    "        \n",
    "        if (thisCoupling in percentTraining['Percentage Training'].values):\n",
    "            pIsPercent.append(True)\n",
    "        else:\n",
    "            pIsPercent.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptrain = np.asarray([pFirstNoun, pFirstAdj, pFirstCD, pSecondNoun, pSecondPerc, pSecondPercSymb])\n",
    "ptrain = ptrain.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "ynb = clf.fit(ptrain, pIsPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppredicted = ynb.predict(ptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptToken = []\n",
    "ptCoupling = []\n",
    "ptFirstNoun = []\n",
    "ptFirstAdj = []\n",
    "ptFirstCD = []\n",
    "ptSecondNoun = []\n",
    "ptSecondPerc = []\n",
    "ptSecondPercSymb = []\n",
    "ptIsPercent = []\n",
    "\n",
    "for sent in np.arange(len(percentSent)):\n",
    "    sentence = percentSent[sent]\n",
    "    sentencePOS = nltk.pos_tag(sentence)\n",
    "    sentLength = len(sentence)\n",
    "            \n",
    "    for word in np.arange(sentLength-1):\n",
    "        thisToken = sentence[word]\n",
    "        nextToken = sentence[word+1]\n",
    "        ptToken.append(thisToken)\n",
    "        \n",
    "        if (nextToken == \"%\"):\n",
    "            thisCoupling = thisToken + nextToken\n",
    "        else:\n",
    "            thisCoupling = thisToken + \" \" + nextToken\n",
    "        ptCoupling.append(thisCoupling)\n",
    "        \n",
    "        FirstPOS = sentencePOS[word][1]\n",
    "        SecondPOS = sentencePOS[word+1][1]\n",
    "\n",
    "        ptFirstNoun.append(FirstPOS == \"NN\")\n",
    "        ptFirstAdj.append(FirstPOS == \"JJ\")\n",
    "        ptFirstCD.append(FirstPOS == \"CD\")\n",
    "        ptSecondNoun.append(SecondPOS == \"NN\")\n",
    "        ptSecondPerc.append(nextToken.lower() == \"percent\")\n",
    "        ptSecondPercSymb.append(nextToken == \"%\")\n",
    "        \n",
    "        if (thisCoupling in percentTraining['Percentage Training'].values):\n",
    "            ptIsPercent.append(True)\n",
    "        else:\n",
    "            ptIsPercent.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pttest = np.asarray([ptFirstNoun, ptFirstAdj, ptFirstCD, ptSecondNoun, ptSecondPerc, ptSecondPercSymb])\n",
    "pttest = pttest.transpose()\n",
    "ptpredicted = ynb.predict(pttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(ptIsPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FP = 0\n",
    "FN = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "\n",
    "for i in np.arange(len(ptpredicted)):\n",
    "    thisPredict = ptpredicted[i]\n",
    "    thisActual = ptIsPercent[i]\n",
    "    if thisPredict and thisActual:\n",
    "        TP = TP + 1\n",
    "    elif thisPredict and (thisActual == False):\n",
    "        FP = FP + 1\n",
    "    elif (thisPredict == False) and thisActual:\n",
    "        FN = FN + 1\n",
    "    elif (thisPredict == False) and (thisActual == False):\n",
    "        TN = TN + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(FP) + \" false positives\\n\" + str(FN) + \" false negatives\\n\" + str(TP) + \" true positives\\n\" + str(TN) + \" true negatives\")\n",
    "print(\"\\naccuracy is \" + str((TP + TN)/(TP + TN + FP + FN)))\n",
    "print(\"precision is \" + str(TP/(TP + FP)))\n",
    "print(\"recall is \" + str(TP/(TP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedPercents = []\n",
    "\n",
    "for i in np.arange(len(ptpredicted)):\n",
    "    if (ptpredicted[i] == True):\n",
    "        predictedPercents.append(ptCoupling[i])\n",
    "\n",
    "PercentsList = f7(predictedPercents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputFile = open(\"./ShipleyPredictedPercents.txt\", \"w\", encoding = 'latin-1')\n",
    "for item in PercentsList:\n",
    "    outputFile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXTRACT SENTENCES WITH COMMON COMPANY SUFFIXES\n",
    "# We know that a percentage will have either the percent sign (%) or the word percent, extract these sentences\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "suffixes = re.compile('| '.join(companySuffixes['Suffixes']))\n",
    "companySent = []\n",
    "\n",
    "for newsDate in np.arange(len(texts2013)):\n",
    "    currentText = texts2013[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    for j in np.arange(len(currentSentences)):\n",
    "        thisSentence = currentSentences[j]\n",
    "        m = re.search(suffixes, thisSentence)\n",
    "        if (m != None):\n",
    "            thisSentence = nltk.word_tokenize(thisSentence)\n",
    "            thisSentence = [word for word in thisSentence if not word in stop]\n",
    "            companySent.append(thisSentence)\n",
    "            \n",
    "for newsDate in np.arange(len(texts2014)):\n",
    "    currentText = texts2014[newsDate]\n",
    "    currentSentences = nltk.sent_tokenize(currentText)\n",
    "    for j in np.arange(len(currentSentences)):\n",
    "        thisSentence = currentSentences[j]\n",
    "        m = re.search(suffixes, thisSentence)\n",
    "        if (m != None):\n",
    "            thisSentence = nltk.word_tokenize(thisSentence)\n",
    "            thisSentence = [word for word in thisSentence if not word in stop]\n",
    "            companySent.append(thisSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates trainingSample, used to train the classification model for percentages\n",
    "trainingSample = []\n",
    "\n",
    "for i in np.arange(len(companySent)):\n",
    "    if (np.random.uniform() < .2):\n",
    "        trainingSample.append(companySent[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Token = []\n",
    "Grouping = []\n",
    "Title = []\n",
    "EndSuffix = []\n",
    "PrevCap = []\n",
    "IsAlpha = []\n",
    "Subset = []\n",
    "IsCompany = []\n",
    "\n",
    "suf = companySuffixes[\"Suffixes\"].tolist()\n",
    "compTrainList = companiesTraining['Companies Training'].tolist()\n",
    "\n",
    "for sent in np.arange(len(trainingSample)):\n",
    "    sentence = trainingSample[sent]\n",
    "    sentLength = len(sentence)\n",
    "    \n",
    "    for word in np.arange(sentLength-2):\n",
    "        \n",
    "        Token1 = sentence[word]\n",
    "        \n",
    "        if (word <= sentLength - 4):\n",
    "            Token2 = sentence[word+1]\n",
    "            Token3 = sentence[word+2]\n",
    "            Token4 = sentence[word+3]\n",
    "            \n",
    "            Grouping4 = Token1 + \" \" + Token2 + \" \" + Token3 + \" \" + Token4\n",
    "            Title4 = Grouping4.istitle()\n",
    "            EndSuffix4 = (Token4 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap4 = False\n",
    "            else:\n",
    "                PrevCap4 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping4)\n",
    "            Title.append(Title4)\n",
    "            EndSuffix.append(EndSuffix4)\n",
    "            PrevCap.append(PrevCap4)\n",
    "            IsCompany.append(Grouping4 in compTrainList)\n",
    "            Subset.append(False)\n",
    "            \n",
    "            Grouping3 = Token1 + \" \" + Token2 + \" \" + Token3\n",
    "            Title3 = Grouping3.istitle()\n",
    "            EndSuffix3 = (Token3 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap3 = False\n",
    "            else:\n",
    "                PrevCap3 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha() and Token3.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping3)\n",
    "            Title.append(Title3)\n",
    "            EndSuffix.append(EndSuffix3)\n",
    "            PrevCap.append(PrevCap3)\n",
    "            IsCompany.append(Grouping3 in compTrainList)\n",
    "            Subset.append(Title4)\n",
    "            \n",
    "            Grouping2 = Token1 + \" \" + Token2\n",
    "            Title2 = Grouping2.istitle()\n",
    "            EndSuffix2 = (Token2 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap2 = False\n",
    "            else:\n",
    "                PrevCap2 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha() and Token3.isalpha() and Token4.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping2)\n",
    "            Title.append(Title2)\n",
    "            EndSuffix.append(EndSuffix2)\n",
    "            PrevCap.append(PrevCap2)\n",
    "            IsCompany.append(Grouping2 in compTrainList)\n",
    "            Subset.append(Title4 or Title3)\n",
    "            \n",
    "        elif (word <= sentLength - 3):\n",
    "            Token2 = sentence[word+1]\n",
    "            Token3 = sentence[word+2]\n",
    "            \n",
    "            Grouping3 = Token1 + \" \" + Token2 + \" \" + Token3\n",
    "            Title3 = Grouping3.istitle()\n",
    "            EndSuffix3 = (Token3 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap3 = False\n",
    "            else:\n",
    "                PrevCap3 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping3)\n",
    "            Title.append(Title3)\n",
    "            EndSuffix.append(EndSuffix3)\n",
    "            PrevCap.append(PrevCap3)\n",
    "            IsCompany.append(Grouping3 in compTrainList)\n",
    "            Subset.append(False)\n",
    "            \n",
    "            Grouping2 = Token1 + \" \" + Token2\n",
    "            Title2 = Grouping2.istitle()\n",
    "            EndSuffix2 = (Token2 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap2 = False\n",
    "            else:\n",
    "                PrevCap2 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha() and Token3.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping2)\n",
    "            Title.append(Title2)\n",
    "            EndSuffix.append(EndSuffix2)\n",
    "            PrevCap.append(PrevCap2)\n",
    "            IsCompany.append(Grouping2 in compTrainList)\n",
    "            Subset.append(Title3)\n",
    "            \n",
    "        else:\n",
    "            Token2 = sentence[word+1]\n",
    "            Grouping2 = Token1 + \" \" + Token2\n",
    "            Title2 = Grouping2.istitle()\n",
    "            EndSuffix2 = (Token2 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap2 = False\n",
    "            else:\n",
    "                PrevCap2 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping2)\n",
    "            Title.append(Title2)\n",
    "            EndSuffix.append(EndSuffix2)\n",
    "            PrevCap.append(PrevCap2)\n",
    "            IsCompany.append(Grouping2 in compTrainList)\n",
    "            Subset.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TitleSubset = np.zeros(len(Grouping))\n",
    "for i in np.arange(5, len(Grouping)-6):\n",
    "    for j in [-5, -4, -3, -2, -1, 1, 2, 3, 4, 5]:\n",
    "        if (Grouping[i] in Grouping[i+j]) and Title[i+j]:\n",
    "            TitleSubset[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctrain = np.asarray([Title, EndSuffix, PrevCap, IsAlpha, TitleSubset])\n",
    "ctrain = ctrain.transpose()\n",
    "clf = BernoulliNB()\n",
    "ynb = clf.fit(ctrain, IsCompany)\n",
    "cpredicted = ynb.predict(ctrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Token = []\n",
    "Grouping = []\n",
    "Title = []\n",
    "EndSuffix = []\n",
    "PrevCap = []\n",
    "IsAlpha = []\n",
    "Subset = []\n",
    "IsCompany = []\n",
    "\n",
    "suf = companySuffixes[\"Suffixes\"].tolist()\n",
    "compTrainList = companiesTraining['Companies Training'].tolist()\n",
    "\n",
    "for sent in np.arange(len(companySent)):\n",
    "    sentence = companySent[sent]\n",
    "    sentLength = len(sentence)\n",
    "    \n",
    "    for word in np.arange(sentLength-2):\n",
    "        \n",
    "        Token1 = sentence[word]\n",
    "        \n",
    "        if (word <= sentLength - 4):\n",
    "            Token2 = sentence[word+1]\n",
    "            Token3 = sentence[word+2]\n",
    "            Token4 = sentence[word+3]\n",
    "            \n",
    "            Grouping4 = Token1 + \" \" + Token2 + \" \" + Token3 + \" \" + Token4\n",
    "            Title4 = Grouping4.istitle()\n",
    "            EndSuffix4 = (Token4 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap4 = False\n",
    "            else:\n",
    "                PrevCap4 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping4)\n",
    "            Title.append(Title4)\n",
    "            EndSuffix.append(EndSuffix4)\n",
    "            PrevCap.append(PrevCap4)\n",
    "            IsCompany.append(Grouping4 in compTrainList)\n",
    "            Subset.append(False)\n",
    "            \n",
    "            Grouping3 = Token1 + \" \" + Token2 + \" \" + Token3\n",
    "            Title3 = Grouping3.istitle()\n",
    "            EndSuffix3 = (Token3 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap3 = False\n",
    "            else:\n",
    "                PrevCap3 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha() and Token3.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping3)\n",
    "            Title.append(Title3)\n",
    "            EndSuffix.append(EndSuffix3)\n",
    "            PrevCap.append(PrevCap3)\n",
    "            IsCompany.append(Grouping3 in compTrainList)\n",
    "            Subset.append(Title4)\n",
    "            \n",
    "            Grouping2 = Token1 + \" \" + Token2\n",
    "            Title2 = Grouping2.istitle()\n",
    "            EndSuffix2 = (Token2 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap2 = False\n",
    "            else:\n",
    "                PrevCap2 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha() and Token3.isalpha() and Token4.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping2)\n",
    "            Title.append(Title2)\n",
    "            EndSuffix.append(EndSuffix2)\n",
    "            PrevCap.append(PrevCap2)\n",
    "            IsCompany.append(Grouping2 in compTrainList)\n",
    "            Subset.append(Title4 or Title3)\n",
    "            \n",
    "        elif (word <= sentLength - 3):\n",
    "            Token2 = sentence[word+1]\n",
    "            Token3 = sentence[word+2]\n",
    "            \n",
    "            Grouping3 = Token1 + \" \" + Token2 + \" \" + Token3\n",
    "            Title3 = Grouping3.istitle()\n",
    "            EndSuffix3 = (Token3 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap3 = False\n",
    "            else:\n",
    "                PrevCap3 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping3)\n",
    "            Title.append(Title3)\n",
    "            EndSuffix.append(EndSuffix3)\n",
    "            PrevCap.append(PrevCap3)\n",
    "            IsCompany.append(Grouping3 in compTrainList)\n",
    "            Subset.append(False)\n",
    "            \n",
    "            Grouping2 = Token1 + \" \" + Token2\n",
    "            Title2 = Grouping2.istitle()\n",
    "            EndSuffix2 = (Token2 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap2 = False\n",
    "            else:\n",
    "                PrevCap2 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha() and Token3.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping2)\n",
    "            Title.append(Title2)\n",
    "            EndSuffix.append(EndSuffix2)\n",
    "            PrevCap.append(PrevCap2)\n",
    "            IsCompany.append(Grouping2 in compTrainList)\n",
    "            Subset.append(Title3)\n",
    "            \n",
    "        else:\n",
    "            Token2 = sentence[word+1]\n",
    "            Grouping2 = Token1 + \" \" + Token2\n",
    "            Title2 = Grouping2.istitle()\n",
    "            EndSuffix2 = (Token2 in suf)\n",
    "            if (word == 0):\n",
    "                PrevCap2 = False\n",
    "            else:\n",
    "                PrevCap2 = sentence[word-1].isupper()\n",
    "            IsAlpha.append(Token1.isalpha() and Token2.isalpha())\n",
    "            Token.append(Token1)\n",
    "            Grouping.append(Grouping2)\n",
    "            Title.append(Title2)\n",
    "            EndSuffix.append(EndSuffix2)\n",
    "            PrevCap.append(PrevCap2)\n",
    "            IsCompany.append(Grouping2 in compTrainList)\n",
    "            Subset.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TitleSubset = np.zeros(len(Grouping))\n",
    "for i in np.arange(5, len(Grouping)-6):\n",
    "    for j in [-5, -4, -3, -2, -1, 1, 2, 3, 4, 5]:\n",
    "        if (Grouping[i] in Grouping[i+j]) and Title[i+j]:\n",
    "            TitleSubset[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctest = np.asarray([Title, EndSuffix, PrevCap, IsAlpha, TitleSubset])\n",
    "ctest = ctest.transpose()\n",
    "ctpredicted = ynb.predict(ctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FP = 0\n",
    "FN = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "\n",
    "for i in np.arange(len(ctpredicted)):\n",
    "    thisPredict = ctpredicted[i]\n",
    "    thisActual = IsCompany[i]\n",
    "    if thisPredict and thisActual:\n",
    "        TP = TP + 1\n",
    "    elif thisPredict and (thisActual == False):\n",
    "        FP = FP + 1\n",
    "    elif (thisPredict == False) and thisActual:\n",
    "        FN = FN + 1\n",
    "    elif (thisPredict == False) and (thisActual == False):\n",
    "        TN = TN + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(FP) + \" false positives\\n\" + str(FN) + \" false negatives\\n\" + str(TP) + \" true positives\\n\" + str(TN) + \" true negatives\")\n",
    "print(\"\\naccuracy is \" + str((TP + TN)/(TP + TN + FP + FN)))\n",
    "print(\"precision is \" + str(TP/(TP + FP)))\n",
    "print(\"recall is \" + str(TP/(TP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictedCompanies = []\n",
    "\n",
    "for i in np.arange(len(ctpredicted)):\n",
    "    if (ctpredicted[i] == True):\n",
    "        predictedCompanies.append(Grouping[i])\n",
    "\n",
    "CompanyList = f7(predictedCompanies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputFile = open(\"./ShipleyPredictedCompanies.txt\", \"w\", encoding = 'latin-1')\n",
    "for item in CompanyList:\n",
    "    outputFile.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
